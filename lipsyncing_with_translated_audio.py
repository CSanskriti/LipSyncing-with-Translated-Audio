# -*- coding: utf-8 -*-
"""Lipsyncing_with_Translated_Audio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yvqbgZZPMlCcXi8nk9ItxzY2Nk7-3Gfs

# Audio Extraction from Input Video

Upload the input Video and Put the path to Input Video in "video_url"

Download sample input video - https://drive.google.com/file/d/14fxLZToffabX31Xv6fqcsu6yd7DGwJr2/view?usp=sharing
"""

video_url = "/content/Original_recording5.mp4"

from IPython.display import HTML
from base64 import b64encode

# Assuming you have already defined the variable `video_url`
mp4 = open(video_url, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML(f"""
<video width="40%" height="80%" controls>
    <source src="{data_url}" type="video/mp4">
</video>""")

!pip install moviepy
# Import necessary libraries
from moviepy.editor import *

def mp4_to_mp3(mp4file, mp3file):
    # Load the video clip
    videoclip = VideoFileClip(mp4file)

    # Extract the audio
    audioclip = videoclip.audio

    # Write the audio to an MP3 file
    audioclip.write_audiofile(mp3file)

    # Close the handles
    audioclip.close()
    videoclip.close()

# Example usage
mp4_to_mp3(video_url, "/content/audio.mp3")

from IPython.display import Audio

# Load and play the audio file
audio_file = "/content/audio.mp3"
display(Audio(audio_file))

# prompt: convert the following code to take input from above cell's audio.mp3 file and the initial video file automatically for checking the length , just print the length of both the audio and video files

import moviepy.editor as mpe

# Load the video and audio files
video = mpe.VideoFileClip(video_url)
audio = mpe.AudioFileClip(audio_file)

# Print the lengths of the video and audio files
print(f"Video length: {video.duration:.2f} seconds")
print(f"Audio length: {audio.duration:.2f} seconds")

"""# Autio Translation


1.   Audio Enhancement
2.   Audio Translation in specified language

## 1. Audio Enhancement
"""

!pip install speechbrain

import torch
import torchaudio
from speechbrain.inference.enhancement import SpectralMaskEnhancement

enhance_model = SpectralMaskEnhancement.from_hparams(
    source="speechbrain/metricgan-plus-voicebank",
    savedir="pretrained_models/metricgan-plus-voicebank",
)

# Load and add fake batch dimension
noisy = enhance_model.load_audio(audio_file).unsqueeze(0)

# Add relative length tensor
enhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))

# Saving enhanced signal on disk
torchaudio.save('/content/enhanced_audio.wav', enhanced.cpu(), 16000)

# Load and play the audio file
audio_file = '/content/enhanced_audio.wav'
display(Audio(audio_file))

"""## 2. Audio Translation in specified language"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install git+https://github.com/openai/whisper

import whisper
medium_model = whisper.load_model("medium.en")

out = medium_model.transcribe("/content/audio.mp3")
out['text']
with open("out.txt", "w") as file:
    file.write(out['text'])

# Read the transcribed text from the 'out.txt' file
with open("out.txt", "r") as file:
    transcribed_text = file.read()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !git clone https://github.com/AI4Bharat/IndicTrans2.git

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %cd /content/IndicTrans2/huggingface_interface

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer
# !python3 -c "import nltk; nltk.download('punkt')"
# !python3 -m pip install bitsandbytes scipy accelerate datasets
# !python3 -m pip install sentencepiece
# 
# !git clone https://github.com/VarunGumma/IndicTransTokenizer
# %cd IndicTransTokenizer
# !python3 -m pip install --editable ./
# %cd ..

"""# **IMPORTANT : Restart your run-time first and then run the cells below.**"""

import torch
from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig
from IndicTransTokenizer import IndicProcessor, IndicTransTokenizer

BATCH_SIZE = 10
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
quantization = None

def initialize_model_and_tokenizer(ckpt_dir, direction, quantization):
    if quantization == "4-bit":
        qconfig = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
    elif quantization == "8-bit":
        qconfig = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_8bit_use_double_quant=True,
            bnb_8bit_compute_dtype=torch.bfloat16,
        )
    else:
        qconfig = None

    tokenizer = IndicTransTokenizer(direction=direction)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        ckpt_dir,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
        quantization_config=qconfig,
    )

    if qconfig == None:
        model = model.to(DEVICE)
        if DEVICE == "cuda":
            model.half()

    model.eval()

    return tokenizer, model


def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):
    translations = []
    for i in range(0, len(input_sentences), BATCH_SIZE):
        batch = input_sentences[i : i + BATCH_SIZE]

        # Preprocess the batch and extract entity mappings
        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)

        # Tokenize the batch and generate input encodings
        inputs = tokenizer(
            batch,
            src=True,
            truncation=True,
            padding="longest",
            return_tensors="pt",
            return_attention_mask=True,
        ).to(DEVICE)

        # Generate translations using the model
        with torch.no_grad():
            generated_tokens = model.generate(
                **inputs,
                use_cache=True,
                min_length=0,
                max_length=256,
                num_beams=5,
                num_return_sequences=1,
            )

        # Decode the generated tokens into text
        generated_tokens = tokenizer.batch_decode(generated_tokens.detach().cpu().tolist(), src=False)

        # Postprocess the translations, including entity replacement
        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)

        del inputs
        torch.cuda.empty_cache()

    return translations

en_indic_ckpt_dir = "ai4bharat/indictrans2-en-indic-1B"  # ai4bharat/indictrans2-en-indic-dist-200M
en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, "en-indic", quantization)

ip = IndicProcessor(inference=True)

en_sents = [transcribed_text]

src_lang, tgt_lang = "eng_Latn", "hin_Deva"
hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)

print(f"\n{src_lang} - {tgt_lang}")
for input_sentence, translation in zip(en_sents, hi_translations):
    print(f"{src_lang}: {input_sentence}")
    print(f"{tgt_lang}: {translation}")

# flush the models to free the GPU memory
del en_indic_tokenizer, en_indic_model

# Assuming you have the translated text stored in the variable 'hi_translations'
# You can replace this with your actual translated text

# Save the translations to a file
with open("translated_txt.txt", "w", encoding="utf-8") as file:
    for translation in hi_translations:
        file.write(translation + "\n")

print("Translations saved to 'translated_txt.txt'")







#!pip install googletrans==4.0.0-rc1
!pip install gTTS

#translator = Translator()

from gtts import gTTS
import os

# Read the translated text from the 'translated_text.txt' file
with open("translated_txt.txt", "r") as file:
    translated_text = file.read()

# Initialize gTTS with the translated text (assuming Hindi language)
tts = gTTS(text=translated_text, lang='hi')

# Save the audio file
audio_filename = "translated_audio.mp3"
tts.save(audio_filename)

# Move the audio file to the Colab notebook directory
os.rename(audio_filename, f"/content/{audio_filename}")

print(f"Audio file '{audio_filename}' has been saved in your Colab notebook directory.")

"""# Voice Cloning"""

#!pip install torch torchaudio
#pip install voice-cloning

with open("/content/out.txt", "r") as f:
  text = f.read()
  print(text)

!pip install numpy==1.26.2 librosa==0.10.1

!pip install TTS

import torch
from TTS.api import TTS

# Get device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Init TTS with the multilingual voice cloning model
tts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False).to(device)

# Assuming 'text' contains the text you want to synthesize
# Replace 'text' with your actual text variable or string

# Run TTS and save output to a file
# Set 'speaker_wav' to your target speaker's audio file
# Set 'language' accordingly, here it is set to English ('en')
tts.tts_to_file(text= text, speaker_wav="/content/enhanced_audio.wav", language="en", file_path="output.wav")

# If you want to use the voice conversion model, initialize it like this:
tts_vc = TTS(model_name="voice_conversion_models/multilingual/vctk/freevc24", progress_bar=False).to(device)

# Run voice conversion and save output to a file
# Set 'source_wav' to your source audio file and 'target_wav' to your target speaker's audio file
tts_vc.voice_conversion_to_file(source_wav="/content/translated_audio.mp3", target_wav="/content/enhanced_audio.wav", file_path="cloned_output.wav")

"""# Lip Syncing"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/

ls

!git clone https://github.com/zabique/Wav2Lip

#download the pretrained model
!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'
a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl

# !pip uninstall tensorflow tensorflow-gpu
!cd /content/Wav2Lip && pip install -r requirements.txt

#download pretrained model for face detection
!wget "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -O "/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth"

!pip install -q youtube-dl
!pip install ffmpeg-python
!pip install librosa==0.9.1

#this code for recording audio
"""
To write this piece of code I took inspiration/code from a lot of places.
It was late night, so I'm not sure how much I created or just copied o.O
Here are some of the possible references:
https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/
https://stackoverflow.com/a/18650249
https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/
https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/
https://stackoverflow.com/a/49019356
"""
from IPython.display import HTML, Audio
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

from IPython.display import clear_output
clear_output()
print("\nDone")

!cd /content/Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "/content/Original_recording5.mp4" --audio "/content/cloned_output.wav" --outfile "results/result.mp4"

mp4 = open('/content/Wav2Lip/results/result.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""
<video width="50%" height="90%" controls>
      <source src="{data_url}" type="video/mp4">
</video>""")

# Evaluation Code

import numpy as np
from skimage.metrics import structural_similarity as ssim


import cv2
def ser(audio, video):
    diff = np.abs(audio - video)
    ser = np.mean(diff)
    return ser

def lmer(video_gt, video_pred):
    diff = np.abs(video_gt - video_pred)
    lmer = np.mean(diff)
    return lmer

def avse(audio, video):
    diff = np.abs(audio - video)
    avse = np.max(diff)
    return avse

def psnr(video_gt, video_pred):
    mse = np.mean((video_gt - video_pred) ** 2)
    psnr = 10 * np.log10(1 / mse)
    return psnr


original_video_path = input("Enter the path to the original video: ")
output_video_path = input("Enter the path to the output video: ")

original_video = cv2.VideoCapture(original_video_path)
output_video = cv2.VideoCapture(output_video_path)


original_frames = []
output_frames = []

while original_video.isOpened() and output_video.isOpened():
    ret1, frame1 = original_video.read()
    ret2, frame2 = output_video.read()

    if ret1 and ret2:
        original_frames.append(frame1)
        output_frames.append(frame2)
    else:
        break

original_video.release()
output_video.release()

original_frames = np.array(original_frames)
output_frames = np.array(output_frames)


ser_value = ser(original_frames, output_frames)
lmer_value = lmer(original_frames, output_frames)
avse_value = avse(original_frames, output_frames)
psnr_value = psnr(original_frames, output_frames)
#ssim_value = calculate_ssim(original_frames, output_frames)


print("Syncing Error Rate (SER): ", ser_value)
print("Lip Movement Error Rate (LMER): ", lmer_value)
print("Audio-Visual Syncing Error (AVSE): ", avse_value)
print("Peak Signal-to-Noise Ratio (PSNR): ", psnr_value)
#print("Structural Similarity Index (SSIM): ", ssim_value)